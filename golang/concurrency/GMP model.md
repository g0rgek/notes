```
   ┌──────────────────┐   ┌───────────────────┐ ┌───────────────┐
   │ GLOBAL RUN QUEUE │   │     Netpoller     │ │    Thread     │
   │                  │   │                   │ │ pool (handoff)│
   │ ┌──────┐ ┌──────┐│   │ ┌─────┐  ┌─────┐  │ │ ┌─┐  ┌─┐  ┌─┐ │
   │ │  G7  │ │  G8  ││   │ │ G10 │  │ G11 │  │ │ │T│  │T│  │T│ │
   │ └──────┘ └──────┘│   │ └─────┘  └─────┘  │ │┌───┐┌───┐┌───┐│
   │ ┌──────────────┐ │   └───────────────────┘ ││ M ││ M ││ M ││
   │ │    MUTEX     │ │                         │└───┘└───┘└───┘│
   └─└──────────────┘─┘                         └───────────────┘
   ┌─────────────┐     ┌─────────────┐     ┌─────────────┐       
   │ CPU1(CORE0) │     │ CPU1(CORE1) │     │ CPU1(CORE2) │       
   └────┌──┐─────┘     └────┌──┐─────┘     └────┌──┐─────┘       
 OS     │  │         OS     │  │          OS    │  │             
thread0 │  │        thread1 │  │         thread2│  │             
   ┌────└──┘─────┐     ┌────└──┘─────┐     ┌────└──┘─────┐       
   │   SYSMON    │     │     G1      │     │     G4      │       
   │             │   M1│   running   │   M2│   running   │       
   └─────────────┘     └─────────────┘     └─────────────┘       
                       │┌───────────┐│     │┌───────────┐│       
                       ││    LRQ    ││     ││    LRQ    ││       
                       || lock-free ||     || 256 elems ||
					   ││ ┌──────┐  ││     ││ ┌──────┐  ││       
                       ││ │  G2  │  ││     ││ │  G5  │  ││       
                       ││ └──────┘  ││     ││ └──────┘  ││       
                       ││ ┌──────┐  ││     ││           ││       
                     P1││ │  G3  │  ││   P2││           ││       
                       ││ └──────┘  ││     ││           ││       
                       │└───────────┘│     │└───────────┘│       
                       |┌───────────┐|     |┌───────────┐|
                       || LIFO-slot || 	   || LIFO-slot ||			 
					   |└───────────┘|     |└───────────┘|
					   │┌───────────┐│     │┌───────────┐│       
                       ││stack cache││     ││stack cache││       
                       │└───────────┘│     │└───────────┘│       
                       │┌───────────┐│     │┌───────────┐│       
                       ││malloc cach││     ││malloc cach││           
                       │└───────────┘│     │└───────────┘│       
                       │┌───────────┐│     │┌───────────┐│       
                       ││other cache││     ││other cache││       
                       │└───────────┘│     │└───────────┘│       
                       └─────────────┘     └─────────────┘                  
```
# Процесс и поток
- Приложение - файл, который лежит на диске.
- Процесс - приложение, загруженное в оперативку, код внутри взят на исполнение. У каждого процесса своё адресное пространство [с секциями](memory%20model/Стек%20горутины.md#Общее%20объяснение). Каждый процесс представлен в виде TASK в планировщике для переключения между ними.
- Поток - подмножество  процесса, разделяют между  его исполняемый код и память. У каждого потока есть свой стек, но куча у всех общая. В потоке меньше информации (ProgramCounter, StackPointer). В планировщике представлен в видео TASK, где хранится информация о его состоянии, чтобы снимать или возобновлять исполнение.
# Многозадачность
[Статья](https://habr.com/ru/articles/502506/)
- В OS Linux вытесняющая (preemptive) многозадачность. Планировшик в любой момент может переключить поток на ядре, перед этим сохранив состояние регистров. 
- В Go - кооперативная многозадачность. По коду расставлены метки, когда горутина может быть безопасная снята с выполнения: 
	- пролог вызова функции (runtime.morestack_noctx) 
	- блокировка на [chan](concurrency/chan.md) или [mutex](concurrency/mutex.md)
	- time.Sleep
	- и другие preemption-friendly execution locations
Горутина проверяет флаг stackguard в прологе функции, при блокирующих операциях чтобы безопасно передать управление:
```go
func doSigPreempt(gp *g, ctxt *sigctxt) {	
	if wantAsyncPreempt(gp) && isAsyncSafePoint(...) {		
		// Inject a call to asyncPreempt.		
		ctxt.pushCall(funcPC(asyncPreempt))	
	}
}
```
## Кооперативная многозадачность
Плюсы - горутина снимается с исполнения только в безопасные моменты для [GC](5.%20GC.md). Но если горутина будет крутиться в бесконечном цикле, выполняя cpu-intensive задачи без блокировок - то передачи управления не будет.
## Asynchronous preemptive
С версии `go 1.14` планировщик получил элементы вытесняющей многозадачности и стал  **asynchronous preemptive**. Если горутина выполняется больше 10ms - он снимает её с выполнения:
- [runtime (sysmon)](#Sysmon) отправит syscall SIGURG потоку ОС (на самом деле М)
- в каждом M зарегистрирован обработчик syscall'ов
- горутина снимется только когда дойдет до безопасного состояния 

Очевидно, что при переходе на вытесняющую концепцию сигнал о вытеснении может застать нашу горутину в любом месте. Но авторы GO решили не уходить от safe-points, а объявить safe-points everywhere! Компилятор автоматически вставит `preemption checks` для таких длинных циклов:
```go
func hotCPU() {
    for {
        // Simulating tight loop
    }
}
```
## Запрет для вытеснения
Директива запрещает вытеснять эту функцию (которая работает в горутине):
```go
//go:nosplit
```
# Зачем свой Планировщик?
**В Golang NxM планировщик**: N горутин на M потоках.

1. Не давать разработчику потоки, а только горутины. Они [легковесные](concurrency/goroutine.md#Что%20такое%20горутина%20?##Легковесный%20поток).
2. Эффективное использование ресурсов процессора и кэшей за счет оптимизаций:
	- пул потоков - не тратимся на создание новых, кладем неиспользуемые в пул
	- добавление новых горутин в очередь текущего процессора (P), так как они создаются в рамках  одного контекста - не нужно вымывать кеши.
	- каждый логический процессор (P) имеет свои кэши, меньше синхронизаций и вызова аллокатора.
	- work stealing - логические процессоры не простаивают, а проактивно ищут работу (LRQ, GRQ, Netpoller)
	- эффективная работа с I/O (netpoller)
	- умный мониторинг и принятие решений (sysmon)
# GMP
- **G** - горутина
- **M** - надстройка над потоком OS
- **P** - логический процессор, который:
	1. отвечает за планирование горутин на M
	2. имеет локальную очередь горутин (LRQ) размером 256
	3. LIFO buffer для последней запущенной горутины в цикле
	4. stack/malloc/other caches
	5. Регулируется через `runtime.GOMAXPROCS()`
# LRQ
У каждого P есть LRQ - очередь на 256 элементов, в которой хранятся горутины со статусом Ready- готовые к исполнению.
# LIFO slot
LRQ состоит из двух состовляющих:
- FIFO на 256
- LIFO slot
Если горутина породила 5 горутин, то 4 попадут в FIFO,  а пятая в LIFO слот. И первой будет исполняться из LIFO слота.
## Зачем
- Очень часто бывает, что горутина порождает только одну горутину, 
- Новую горутину желательно сразу же начать исполнять
- И не дать другому P её своровать, так если добавить в конец FIFO - её могут своровать 
## Разделение общего времени выполнения
- Горутина G2 породила G5, и передала управление
- G5 после выполнения передаст управление G2
- Получается, круговая передача управления и потенциальное голодание других горутин
- Поэтому между G2 и G5 будет разделяться предельное время выполнения - 10ms
- В итоге, одна из горутин будет вытеснена в [GRQ](#GRQ)
# Work stealing
scheduler_tick - счетчик, увеличивается на 1 когда планировшик ставит новую горутину на исполнение.
## Алгоритм
Если LRQ у P опустела, он: 
0. Если это 1/61 scheduler_tick, то P заберет половину из [GRQ](GRQ). Если нет - к шагу 1.
1. рандомно P, чтобы забрать **с конца** его [LRQ](LRQ) **половину** горутин. Это позволяет минимизировать contention rate и забрать достаточное количество работы себе и оставить изначальному P. LRQ представлены в виде Lock-free очереди, без [mutex](concurrency/mutex.md).
2. Если не получилось украсть - он попробует украсть еще 4 раза.
3. Если опять не получилось - идет в GRQ
4. Проверит [Netpoller](Netpoller) и заберет готовые горутины
## Запуск алгоритма
Изначально создаётся GOMAXPROCS P. Один из них активный, выполняет код горутины main и плодит новые горутины себе в очередь. Ещё один находится в состоянии "spinning", то есть активно пытается выполнять work stealing. Остальные P неактивны. Когда второй смог что-то своровать у первого, он тоже становится активным, и в этот момент один из неактивных переходит в состояние "spinning", потому что в этом состоянии всегда должен быть хотя бы один неактивный P. Процесс повторяется пока все P не получат работу. 
# Wait Reason
[Source code](https://go.dev/src/runtime/runtime2.go#:~:text=type%20waitReason%20uint8)
Так как планировщик реализует [кооперативную многозадачность](##Кооперативная%20многозадачность), когда горутина переходит в статус waiting, у нее выставляется определенный
```go
type waitReason uint8
```
# GRQ
- Если горутина не смогла вернуться в [LRQ](#LRQ) своего/рандомного P - она попадает в Global Run Queue
- Если горутина исполняется дольше 10ms - [sysmon](#Sysmon) вытесняет её в конец GRQ
- Доступ к GRQ синхронизируется через [mutex](concurrency/mutex.md)
- P ходят в GRQ каждый 1/61 тик планирования [work stealing](#Work%20stealing)
# Sysmon
Принимает решения в планировщике:
- Следит за временем выполнения горутин, отправляет SIGURG для вытеснения, помещает в [GRQ](#GRQ)
- Производит handoff при [synchronous syscall](#Synchronous%20syscall) 
- Собирает метрики
- Еще что-нибудь...
# Synchronous syscall
0. Синхронные syscall - чтение с диска
1. При вызове синхронного syscall, OS thread уходит в kernel space и в месте с горутиной блокируется на долгое время.
2. [Sysmon](#Sysmon) понимает, что горутина блокируется на сисколе, снимает (OS thread-M) с логического процессора P. 
3. **Handoff**: runtime запрашивает новый OS thread с M, присваивает его прежнему P
- Оптимизация: если syscall бытрый, то **handoff'a** не будет. Но если планировщик ошибется и горутина заблокируется >10ms - создание os thread всё-таки произойдет.
4. Когда syscall отработает - планировщик попробует вернуть горутину:
	- Изначальному P
	- Если занято, то рандомному P
	- Если не получилось, то в [GRQ](#GRQ)
	OS thread планировщик положит в thread pool.
5. Если происходит много синхронных сисколов - будут аллоцироваться новые os thread, приведет к сильному расходу памяти. Нужно ограничивать кол-во горутин, которые работают с таким типом сисколов.
# Asynchronous syscall
- Асинхронные syscall - отправка сетевых запросов
- Горутины снимаются с выполнения, передаются [Netpoller](#Netpoller)
# Netpoller
## Visual
```
                      sock│                               
                       92 │OS                             
                      sock│ ▲                             
                       52 │ │socket()                     
         ┌───────────┐────▼─┼──────┬─────────────────────┐
         │  ┌────┐   │  call│stack │    callback queue   │
         │  │ G1 │   │      │      │┌───────────────────┐│
         │  └────┘   │      │      ││G4: on sock create ││
         │  ┌────┐   │      │      ││     connect()     ││
         │  │ G4 │   │      │      │└───────────────────┘│
         │  └────┘   │┌─────┴─────┐│┌───────────────────┐│
         │           ││G4:socket()│││G1: on sock create ││
         │           │└───────────┘││     connect()     ││
         │           │┌───────────┐│└───────────────────┘│
         │           ││G1:socket()││                     │
         │           │└───────────┘│                     │
         └───────────┘─────────────┴─────────────────────┐
         │                 OS thread                     │
         └───────────────────────────────────────────────┘                       
-----------------------------------------------------------------------------    
                         OS                               
                        │ ▲connect(52)-async, dont wait   
                        │ │connect(92)-async, dont wait   
       ┌───────────┐────▼─┼──────┬─────────────────────┐  
       │  ┌────┐   │  call│stack │    callback queue   │  
       │  │ G1 │   │      │      │┌───────────────────┐│  
       │  └────┘   │      │      ││G4:on 52 connected ││  
       │  ┌────┐   │      │      ││      send()       ││  
       │  │ G4 │   │      │      │└───────────────────┘│  
       │  └────┘   │┌─────┴─────┐│┌───────────────────┐│  
       │           ││G4:conn(52)│││G1:on 92 connected ││  
       │           │└───────────┘││      send()       ││  
       │           │┌───────────┐│└───────────────────┘│  
       │           ││G1:conn(92)││                     │  
       │           │└───────────┘│                     │  
       └───────────┘─────────────┴─────────────────────┐  
       │                 OS thread                     │  
       └───────────────────────────────────────────────┘
-----------------------------------------------------------------------------  
                         OS                               
                          ▲                               
                          │epoll_add(52,92)               
       ┌───────────┐──────┼──────┬─────────────────────┐  
       │  ┌────┐   │  call stack │    callback queue   │  
       │  │ G1 │   │             │┌───────────────────┐│  
       │  └────┘   │             ││G4:on 52 connected ││  
       │  ┌────┐   │             ││      send()       ││  
       │  │ G4 │   │             │└───────────────────┘│  
       │  └────┘   │             │┌───────────────────┐│  
       │           │             ││G1:on 92 connected ││  
       │           │             ││      send()       ││  
       │           │             │└───────────────────┘│  
       │           │             │                     │  
       │           │             │                     │  
       └───────────┘─────────────┴─────────────────────┐  
       │                 OS thread                     │  
       └───────────────────────────────────────────────┘
-----------------------------------------------------------------------------  
          ready to recv: OS                               
                 52     │ ▲                               
                 92     │ │epoll_wait()                   
       ┌───────────┐────▼─┼──────┬─────────────────────┐  
       │  ┌────┐   │  call stack │    callback queue   │  
       │  │ G1 │   │             │                     │  
       │  └────┘   │             │                     │  
       │  ┌────┐   │             │                     │  
       │  │ G4 │   │             │                     │  
       │  └────┘   │             │                     │  
       │           │             │                     │  
       │           │             │                     │  
       │           │             │                     │  
       │           │             │                     │  
       │           │             │                     │  
       └───────────┘─────────────┴─────────────────────┐  
       │                 OS thread                     │  
       └───────────────────────────────────────────────┘
----------------------------------------------------------------------------- 
                         OS                               
                    data│ ▲ recv(52)                      
                        │ │ recv(92)                      
       ┌───────────┐────▼─┼──────┬─────────────────────┐  
       │  ┌────┐   │  call│stack │    callback queue   │  
       │  │ G1 │   │      │      │┌───────────────────┐│  
       │  └────┘   │      │      ││G4:on 52 on recv() ││  
       │  ┌────┐   │      │      ││   set runnable    ││  
       │  │ G4 │   │      │      │└───────────────────┘│  
       │  └────┘   │┌─────┴─────┐│┌───────────────────┐│  
       │           ││G4:recv(52)│││G1:on 92 on recv() ││  
       │           │└───────────┘││   set runnable    ││  
       │           │┌───────────┐│└───────────────────┘│  
       │           ││G1:recv(92)││                     │  
       │           │└───────────┘│                     │  
       └───────────┘─────────────┴─────────────────────┐  
       │                 OS thread                     │  
       └───────────────────────────────────────────────┘
-----------------------------------------------------------------------------  
       ┌───────────┐─────────────┬─────────────────────┐  
       │  ┌────┐   │  call stack │    callback queue   │  
       │  │ G1 │   │             │┌───────────────────┐│  
       │  └────┘   │             ││G1:on 92 on recv() ││  
back to│  ┌────┐   │             ││   set runnable    ││  
   P◄──┼──┼ G4 │   │             │└───────────────────┘│  
  LRQ  │  └────┘   │             │                     |
  or   │           │             │                     |
  GRQ  │           │             │                     |
       │           │┌───────────┐│                     │  
       │           ││G1:recv(92)││                     │  
       │           │└───────────┘│                     │  
       └───────────┘─────────────┴─────────────────────┐  
       │                 OS thread                     │  
       └───────────────────────────────────────────────┘  
```
## Алгоритм
- Под netpoller создается отдельный OS thread, и все горутины паркуются на этом треде
- Если запустить 100 горутин, то не будет создавать 100 потоков - все будут перенеси в netpoller
- Под каждый запрос создается сокет - файловый дескриптор. В ОС есть ограничение на кол-во дескрипторов, можно снять через `ulimit -s unlimited`

